{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload after code has changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running first time, moving up one dir level\n",
      "This path should be the root directory of the project:  /home/daniel/Documents/TUM/ADLR/tum-adlr-02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# move into the correct dirrectory, e.g. move up one directory level iif this cell is run for the first time\n",
    "try:\n",
    "    a = first_time\n",
    "except NameError:\n",
    "    print(\"Running first time, moving up one dir level\")\n",
    "    os.chdir('..')  # Move up one directory level to the root directory of project\n",
    "    first_time = False\n",
    "\n",
    "print(\"This path should be the root directory of the project: \", os.getcwd())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "Creating the dataset object and applzing transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class mug already downloaedd. Skipping download.\n",
      "class bottle already downloaedd. Skipping download.\n",
      "2D images for classmug already exist. Skipping conversion.\n",
      "2D images for classbottle already exist. Skipping conversion.\n",
      "Generating annotation CSV files for training and testing with a split ratio of 0.9:0.1.\n",
      "Finished generating annotation CSV files for 71200 different shapes.\n"
     ]
    }
   ],
   "source": [
    "from data.model_classes import Mug, Bottle\n",
    "from data.dataconverter import DataConverter\n",
    "\n",
    "# generate data\n",
    "dataconverter = DataConverter(\n",
    "    classes=[Mug(),Bottle()],\n",
    "    min_order = 1,\n",
    "    tact_order = 5,\n",
    "    tact_number=1\n",
    "    \n",
    ")\n",
    "# set regenerate to true, if you run this after changes in dataconverter have been made\n",
    "dataconverter.generate_2d_dataset(show_results=False, regenerate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/adlr/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from data.reconstruction_dataset import *\n",
    "\n",
    "csv_file = './datasets/2D_shapes/annotations.csv'\n",
    "root_dir = './datasets/2D_shapes'\n",
    "composed = transforms.Compose([RandomOrientation(),\n",
    "                               ToTensor()])\n",
    "\n",
    "dataset = ReconstructionDataset(csv_file=csv_file,\n",
    "                                root_dir=root_dir,\n",
    "                                transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examplary data pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_data = dataset[5]\n",
    "\n",
    "plt.figure()\n",
    "print(example_data['image'])\n",
    "show_datapair(example_data['image'], example_data['label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Hyperparamters\n",
    "\n",
    "Look into the file neural_nets.trainer to see which hyperparameters you can choose.\n",
    "The seperation into tunable and non tunable hyperparameters is made, because this makes parameter searches with ray easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Tunable Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets.trainer import NonTHparams\n",
    "\n",
    "\n",
    "nt_h = NonTHparams()\n",
    "nt_h.num_epochs = 50\n",
    "nt_h.train_prop = 0.97 # set way to high to make validation period short and make testing this search easier\n",
    "\n",
    "nt_h.print_log = False # to better see param search results\n",
    "nt_h.log_train_period = 1\n",
    "nt_h.log_val_period = 5 #set low to test this parameter search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tunable Hyperparameters and parameter spaces for upgrade\n",
    "\n",
    "The hyperparameters we want to tone have to be put into a list of possible values and that list into a dict, for the hyperparameter optimizer to do its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from neural_nets.trainer import THparams\n",
    "from neural_nets.weight_inits import weight_init_kx\n",
    "from neural_nets.models.unet import UNet2\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "t_h = THparams()\n",
    "t_h.batch_size = 16\n",
    "\n",
    "t_h.model = UNet2\n",
    "t_h.weight_init = weight_init_kx\n",
    "t_h.depth = 5\n",
    "t_h.channels = 64\n",
    "\n",
    "t_h.lr = 1e-4\n",
    "t_h.optimizer = optim.Adam\n",
    "t_h.loss_func = nn.BCELoss()\n",
    "\n",
    "# config is the set of params, that will be searched, they got to ghave the same key names, as variables in THparams\n",
    "config = {\n",
    "    \"batch_size\": tune.choice([2 ** i for i in range(1,5)]),\n",
    "    \"depth\": tune.choice([i for i in range(2,9)]),\n",
    "    \"lr\": tune.loguniform(1e-7, 1e-2),\n",
    "    \"depth\": tune.choice([2,3,4,5,6,7,8,9]),\n",
    "    \"channels\": tune.choice([2 ** i for i in range(2,7)])\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During training, progress will be logged to tensorboard. Go to project folder, activate appropritae conda environment and run 'tensorboard --logdir runs/' to see the logs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/TUM/ADLR/tum-adlr-02/neural_nets/trainer.py:111: UserWarning: Cuda is not available, device used instead: cpu\n",
      "  warnings.warn(\"Cuda is not available, device used instead: \" + str(self.device))\n"
     ]
    }
   ],
   "source": [
    "from neural_nets.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(nt_h, t_h, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 03:34:44,325\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-06-22 03:34:44,327\tINFO registry.py:106 -- Detected unknown callable for trainable. Converting to class.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-22 03:37:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:12.35        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.5/15.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  channels</th><th style=\"text-align: right;\">  depth</th><th style=\"text-align: right;\">         lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_from_dict_9a68f_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">3.03625e-06</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">0.000755535</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">3.16361e-07</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">1.79448e-06</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">2.84501e-07</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">0.000461926</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">0.000189069</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">7.53361e-05</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">      7</td><td style=\"text-align: right;\">1.14007e-05</td></tr>\n",
       "<tr><td>train_from_dict_9a68f_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">0.000323696</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_from_dict pid=13487)\u001b[0m Epoch [1/50], Step [10/15539], Train Loss: 0.2857 , Logging Time Proportion: 0.0013, Data Loading Time Proportion: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_samples=10\n",
    "max_num_epochs=3 # it is not num epochs, but how often we calculae val loss (we can also calc it mid epoch) todo: make it more logical\n",
    "gpus_per_trial = 1\n",
    "\n",
    "os.environ[\"RAY_CHDIR_TO_TRIAL_DIR\"] = \"0\"\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "result = tune.run(\n",
    "        trainer.train_from_dict,\n",
    "        resources_per_trial={\"cpu\": 10, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
