{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload after code has changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This path should be the root directory of the project:  /home/daniel/Documents/TUM/ADLR/tum-adlr-02\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# move into the correct dirrectory, e.g. move up one directory level iif this cell is run for the first time\n",
    "try:\n",
    "    a = first_time\n",
    "except NameError:\n",
    "    print(\"Running first time, moving up one dir level\")\n",
    "    os.chdir('..')  # Move up one directory level to the root directory of project\n",
    "    first_time = False\n",
    "\n",
    "print(\"This path should be the root directory of the project: \", os.getcwd())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset\n",
    "Creating the dataset object and applzing transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class mug already downloaedd. Skipping download.\n",
      "class bottle already downloaedd. Skipping download.\n",
      "2D images for classmug already exist. Skipping conversion.\n",
      "2D images for classbottle already exist. Skipping conversion.\n",
      "Generating annotation CSV files for training and testing with a split ratio of 0.9:0.1.\n",
      "Finished generating annotation CSV files for 71200 different shapes.\n"
     ]
    }
   ],
   "source": [
    "from data.model_classes import Mug, Bottle\n",
    "from data.dataconverter import DataConverter\n",
    "\n",
    "# generate data\n",
    "dataconverter = DataConverter(\n",
    "    classes=[Mug(),Bottle()],\n",
    "    min_order = 1,\n",
    "    tact_order = 2,\n",
    "    tact_number=1\n",
    "    \n",
    ")\n",
    "# set regenerate to true, if you run this after changes in dataconverter have been made\n",
    "dataconverter.generate_2d_dataset(show_results=False, regenerate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.reconstruction_dataset import *\n",
    "\n",
    "csv_file = './datasets/2D_shapes/annotations.csv'\n",
    "root_dir = './datasets/2D_shapes'\n",
    "composed = transforms.Compose([RandomOrientation(),\n",
    "                               ToTensor()])\n",
    "\n",
    "dataset = ReconstructionDataset(csv_file=csv_file,\n",
    "                                root_dir=root_dir,\n",
    "                                transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examplary data pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAEMCAYAAABZZbUfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIHklEQVR4nO3dW4icdxnH8WcnJ7KpJJ6gWgvVNlZbRaltSSyoVUJLBBtLFwq10AuRRBQlKHrjjVd6441Q1FBBPMYoVC+sGor2wh5E01Z7MIeCxFpNA4akOM1md+b1quB2wu5mM7vvzvw+n8tn/+87z91852V2d6JpmqYAgFidthcAANolBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIt3axB3d0ppZzDwBgGRzsH1jwjCcDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhBMDABBODABAODEAAOHEAACEEwMAEE4MAEA4MQAA4cQAAIQTAwAQTgwAQDgxAADhxAAAhFvb9gIAq8GaLZvbXmFVas7NVL/bbXsNlpkYWEan7tle67pNXfLTR+c9d+Q7N9Q7v3C4emfOLOl1mu3vqRc+uKku+9rDS7p+PhPr1teRfe+urff8eej3htWiMzlZv3rmobbXWJU+8szHasNdZwfmzekz1T87OGc0iYFltOXoy9U516tmgXNveHRt9aenl/w6606crtc/tWHJ18+n6fXqdX9Yvyz3Bla/B6/5ZdXjg/Orv7unrtr3/JxZ74UT1cycW6HNGKaJpmkWeq+qqqodnanl3gWgFZ3JyXrg2PCfrKV5/97dteXQyYF578hzLWzDKw72Dyx4xpMBAIbi4W9867zzW3bdXZ3p2bnDpqn+X/62AluxGGIAgGX1m/u/PzA73X+5pqZ2z5mt6Z6r/pPPrtRa/B8xAMCK29zZWL/9+ffmzH7y0mtr357bB85u+NeZ6j17dKVWiyQGAFgV7nzNqbrzB/cNzD/01K5qvnljbTryn+odPlad915T3csvmfdeG3/9hC8zXgAxAMCq9vt33V/17aprH7mrtuzfVpv3HK+Hrv7RvNd89LpbavbfJ1ZmwTEgBgAYCU9v/2HV9ra3GE/+HDEAY+f4J66sibU+7y6WGABg7Px17731jy/eWNVZ0/YqI0EMADCWnv7svdVZv67tNUaCGACAcGIAAMKJAQAIJwYugG+mAjCOxMAidSYn66tHH2l7DQAYOjGwSP1ut77y1hvaXgMAhk4MXIC1b7q07RUAYOjEwCJ1Jidr6neH2l4DAIZODCxSv9utH7/jzW2vAQBDJwYAIJwYAIBwYgAAwokBAAgnBgAYSztvvqP6Z8+2vcZIEAMAjJ2dN99RvcPH2l5jZIgBAMbOxOmX2l5hpPjPOwCMhCv3766tX368qqpePHBFHbp+f1VVfeDTn6rJB56cc7aZPrHi+40yMQDAqnP7sR3V/fCpObOreo9V0zRVVfXG247UrWuur6qqjbN/rGbFNxwvYgCA1jwxPV1fetu2wR80J+e/sGmqmZ1dnqUCiQEAhq7X9M8733nZdeeZ+lzfNjEAwEWZaXoDs103fbxm/368hW1YCjEAwKLMNL2abmYG5rd+/nO16WePvWoqBEaJGLhIE++7tg7v2Vhv/+SfLv5mD76lJnaerGZ6euVfG8K92Ptv2yusett+sbe2fubVb/pVm2pwxmgRAxep89zztfW+K4Zyr5mvX1rrzv2zldeGZP1ut+6+/Ka211j1tnrTH1sTzSu/p7GAHZ2p5d4FABiyg/0DC57xFwgBIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAgnBgAgHBiAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwokBAAgnBgAg3ETTNE3bSwAA7fFkAADCiQEACCcGACCcGACAcGIAAMKJAQAIJwYAIJwYAIBwYgAAwv0PNZkOBItIzN0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_data = dataset[5]\n",
    "\n",
    "plt.figure()\n",
    "print(example_data['image'])\n",
    "show_datapair(example_data['image'], example_data['label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Hyperparamters\n",
    "\n",
    "Look into the file neural_nets.trainer to see which hyperparameters you can choose.\n",
    "The seperation into tunable and non tunable hyperparameters is made, because this makes parameter searches with ray easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Tunable Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets.trainer import NonTHparams\n",
    "\n",
    "\n",
    "nt_h = NonTHparams()\n",
    "nt_h.num_epochs = 50\n",
    "nt_h.train_prop = 0.9\n",
    "\n",
    "nt_h.log_train_period = 100\n",
    "nt_h.log_val_period = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunable Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.named_parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m t_h\u001b[38;5;241m.\u001b[39mloss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     15\u001b[0m t_h\u001b[38;5;241m.\u001b[39mweight_init \u001b[38;5;241m=\u001b[39m weight_init_kx\n\u001b[0;32m---> 18\u001b[0m \u001b[43mcount_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TUM/ADLR/tum-adlr-02/neural_nets/models/unet.py:11\u001b[0m, in \u001b[0;36mcount_parameters\u001b[0;34m(model, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_parameters\u001b[39m(model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m     total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.named_parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from neural_nets.trainer import THparams\n",
    "from neural_nets.weight_inits import weight_init_kx\n",
    "from neural_nets.models.unet import UNet1, count_parameters\n",
    "\n",
    "\n",
    "t_h = THparams()\n",
    "t_h.batch_size = 16\n",
    "t_h.lr = 1e-4\n",
    "t_h.model = UNet1\n",
    "t_h.optimizer = optim.Adam\n",
    "t_h.loss_func = nn.BCELoss()\n",
    "t_h.weight_init = weight_init_kx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During training, progress will be logged to tensorboard. Go to project folder, activate appropritae conda environment and run 'tensorboard --logdir runs/' to see the logs.\n"
     ]
    }
   ],
   "source": [
    "from neural_nets.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(nt_h, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.691719114780426\n",
      "Epoch [1/50], Step [1/3204], Train Loss: 0.6917 , Logging Time Proportion: 0.0160, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.6703320741653442\n",
      "Epoch [1/50], Step [2/3204], Train Loss: 0.6703 , Logging Time Proportion: 0.0322, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.6240219473838806\n",
      "Epoch [1/50], Step [3/3204], Train Loss: 0.6240 , Logging Time Proportion: 0.0353, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.5343635082244873\n",
      "Epoch [1/50], Step [4/3204], Train Loss: 0.5344 , Logging Time Proportion: 0.0467, Data Loading Time Proportion: 0.0028\n",
      "loss:  0.5776985883712769\n",
      "Epoch [1/50], Step [5/3204], Train Loss: 0.5777 , Logging Time Proportion: 0.0332, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.480016827583313\n",
      "Epoch [1/50], Step [6/3204], Train Loss: 0.4800 , Logging Time Proportion: 0.0353, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.4214510917663574\n",
      "Epoch [1/50], Step [7/3204], Train Loss: 0.4215 , Logging Time Proportion: 0.0332, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.41512155532836914\n",
      "Epoch [1/50], Step [8/3204], Train Loss: 0.4151 , Logging Time Proportion: 0.0499, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.3767635226249695\n",
      "Epoch [1/50], Step [9/3204], Train Loss: 0.3768 , Logging Time Proportion: 0.0369, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.34828507900238037\n",
      "Epoch [1/50], Step [10/3204], Train Loss: 0.3483 , Logging Time Proportion: 0.0358, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.3667903542518616\n",
      "Epoch [1/50], Step [11/3204], Train Loss: 0.3668 , Logging Time Proportion: 0.0337, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.3085431456565857\n",
      "Epoch [1/50], Step [12/3204], Train Loss: 0.3085 , Logging Time Proportion: 0.0334, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.29076239466667175\n",
      "Epoch [1/50], Step [13/3204], Train Loss: 0.2908 , Logging Time Proportion: 0.0335, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.2976379096508026\n",
      "Epoch [1/50], Step [14/3204], Train Loss: 0.2976 , Logging Time Proportion: 0.0325, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.28306883573532104\n",
      "Epoch [1/50], Step [15/3204], Train Loss: 0.2831 , Logging Time Proportion: 0.0453, Data Loading Time Proportion: 0.0026\n",
      "loss:  0.2783295810222626\n",
      "Epoch [1/50], Step [16/3204], Train Loss: 0.2783 , Logging Time Proportion: 0.0349, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.26141148805618286\n",
      "Epoch [1/50], Step [17/3204], Train Loss: 0.2614 , Logging Time Proportion: 0.0310, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.26725149154663086\n",
      "Epoch [1/50], Step [18/3204], Train Loss: 0.2673 , Logging Time Proportion: 0.0319, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.2579417824745178\n",
      "Epoch [1/50], Step [19/3204], Train Loss: 0.2579 , Logging Time Proportion: 0.0351, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.23077403008937836\n",
      "Epoch [1/50], Step [20/3204], Train Loss: 0.2308 , Logging Time Proportion: 0.0348, Data Loading Time Proportion: 0.0029\n",
      "loss:  0.23631435632705688\n",
      "Epoch [1/50], Step [21/3204], Train Loss: 0.2363 , Logging Time Proportion: 0.0367, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.22932571172714233\n",
      "Epoch [1/50], Step [22/3204], Train Loss: 0.2293 , Logging Time Proportion: 0.0305, Data Loading Time Proportion: 0.0016\n",
      "loss:  0.2186463177204132\n",
      "Epoch [1/50], Step [23/3204], Train Loss: 0.2186 , Logging Time Proportion: 0.0302, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.22191846370697021\n",
      "Epoch [1/50], Step [24/3204], Train Loss: 0.2219 , Logging Time Proportion: 0.0331, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.20807328820228577\n",
      "Epoch [1/50], Step [25/3204], Train Loss: 0.2081 , Logging Time Proportion: 0.0310, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.1978316307067871\n",
      "Epoch [1/50], Step [26/3204], Train Loss: 0.1978 , Logging Time Proportion: 0.0311, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.20798425376415253\n",
      "Epoch [1/50], Step [27/3204], Train Loss: 0.2080 , Logging Time Proportion: 0.0327, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.19300585985183716\n",
      "Epoch [1/50], Step [28/3204], Train Loss: 0.1930 , Logging Time Proportion: 0.0315, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.18871912360191345\n",
      "Epoch [1/50], Step [29/3204], Train Loss: 0.1887 , Logging Time Proportion: 0.0319, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.1766185760498047\n",
      "Epoch [1/50], Step [30/3204], Train Loss: 0.1766 , Logging Time Proportion: 0.0317, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.18558230996131897\n",
      "Epoch [1/50], Step [31/3204], Train Loss: 0.1856 , Logging Time Proportion: 0.0261, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.17395560443401337\n",
      "Epoch [1/50], Step [32/3204], Train Loss: 0.1740 , Logging Time Proportion: 0.0304, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.16862884163856506\n",
      "Epoch [1/50], Step [33/3204], Train Loss: 0.1686 , Logging Time Proportion: 0.0341, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.1666773110628128\n",
      "Epoch [1/50], Step [34/3204], Train Loss: 0.1667 , Logging Time Proportion: 0.0280, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.1589270532131195\n",
      "Epoch [1/50], Step [35/3204], Train Loss: 0.1589 , Logging Time Proportion: 0.0379, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.1543882191181183\n",
      "Epoch [1/50], Step [36/3204], Train Loss: 0.1544 , Logging Time Proportion: 0.0254, Data Loading Time Proportion: 0.0026\n",
      "loss:  0.16442550718784332\n",
      "Epoch [1/50], Step [37/3204], Train Loss: 0.1644 , Logging Time Proportion: 0.0307, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.1441386640071869\n",
      "Epoch [1/50], Step [38/3204], Train Loss: 0.1441 , Logging Time Proportion: 0.0238, Data Loading Time Proportion: 0.0015\n",
      "loss:  0.15601924061775208\n",
      "Epoch [1/50], Step [39/3204], Train Loss: 0.1560 , Logging Time Proportion: 0.0374, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.12994661927223206\n",
      "Epoch [1/50], Step [40/3204], Train Loss: 0.1299 , Logging Time Proportion: 0.0253, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.13846951723098755\n",
      "Epoch [1/50], Step [41/3204], Train Loss: 0.1385 , Logging Time Proportion: 0.0289, Data Loading Time Proportion: 0.0018\n",
      "loss:  0.13113239407539368\n",
      "Epoch [1/50], Step [42/3204], Train Loss: 0.1311 , Logging Time Proportion: 0.0277, Data Loading Time Proportion: 0.0027\n",
      "loss:  0.11748719215393066\n",
      "Epoch [1/50], Step [43/3204], Train Loss: 0.1175 , Logging Time Proportion: 0.0340, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.1194397360086441\n",
      "Epoch [1/50], Step [44/3204], Train Loss: 0.1194 , Logging Time Proportion: 0.0252, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.11226817965507507\n",
      "Epoch [1/50], Step [45/3204], Train Loss: 0.1123 , Logging Time Proportion: 0.0199, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.1136976033449173\n",
      "Epoch [1/50], Step [46/3204], Train Loss: 0.1137 , Logging Time Proportion: 0.0235, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.11658042669296265\n",
      "Epoch [1/50], Step [47/3204], Train Loss: 0.1166 , Logging Time Proportion: 0.0254, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.139582097530365\n",
      "Epoch [1/50], Step [48/3204], Train Loss: 0.1396 , Logging Time Proportion: 0.0205, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.17072972655296326\n",
      "Epoch [1/50], Step [49/3204], Train Loss: 0.1707 , Logging Time Proportion: 0.0265, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.1661035567522049\n",
      "Epoch [1/50], Step [50/3204], Train Loss: 0.1661 , Logging Time Proportion: 0.0252, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.11976560950279236\n",
      "Epoch [1/50], Step [51/3204], Train Loss: 0.1198 , Logging Time Proportion: 0.0222, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.16406027972698212\n",
      "Epoch [1/50], Step [52/3204], Train Loss: 0.1641 , Logging Time Proportion: 0.0174, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.11203070729970932\n",
      "Epoch [1/50], Step [53/3204], Train Loss: 0.1120 , Logging Time Proportion: 0.0207, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.12442390620708466\n",
      "Epoch [1/50], Step [54/3204], Train Loss: 0.1244 , Logging Time Proportion: 0.0193, Data Loading Time Proportion: 0.0017\n",
      "loss:  0.12851345539093018\n",
      "Epoch [1/50], Step [55/3204], Train Loss: 0.1285 , Logging Time Proportion: 0.0246, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.1113433688879013\n",
      "Epoch [1/50], Step [56/3204], Train Loss: 0.1113 , Logging Time Proportion: 0.0247, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.11935385316610336\n",
      "Epoch [1/50], Step [57/3204], Train Loss: 0.1194 , Logging Time Proportion: 0.0215, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.11477115005254745\n",
      "Epoch [1/50], Step [58/3204], Train Loss: 0.1148 , Logging Time Proportion: 0.0213, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.10357847809791565\n",
      "Epoch [1/50], Step [59/3204], Train Loss: 0.1036 , Logging Time Proportion: 0.0224, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.10764385759830475\n",
      "Epoch [1/50], Step [60/3204], Train Loss: 0.1076 , Logging Time Proportion: 0.0227, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.10909315943717957\n",
      "Epoch [1/50], Step [61/3204], Train Loss: 0.1091 , Logging Time Proportion: 0.0202, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.10630278289318085\n",
      "Epoch [1/50], Step [62/3204], Train Loss: 0.1063 , Logging Time Proportion: 0.0204, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.10846041142940521\n",
      "Epoch [1/50], Step [63/3204], Train Loss: 0.1085 , Logging Time Proportion: 0.0207, Data Loading Time Proportion: 0.0026\n",
      "loss:  0.1094445213675499\n",
      "Epoch [1/50], Step [64/3204], Train Loss: 0.1094 , Logging Time Proportion: 0.0197, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.10632637143135071\n",
      "Epoch [1/50], Step [65/3204], Train Loss: 0.1063 , Logging Time Proportion: 0.0220, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.10717268288135529\n",
      "Epoch [1/50], Step [66/3204], Train Loss: 0.1072 , Logging Time Proportion: 0.0226, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.09970510005950928\n",
      "Epoch [1/50], Step [67/3204], Train Loss: 0.0997 , Logging Time Proportion: 0.0193, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.09640374779701233\n",
      "Epoch [1/50], Step [68/3204], Train Loss: 0.0964 , Logging Time Proportion: 0.0333, Data Loading Time Proportion: 0.0027\n",
      "loss:  0.10325392335653305\n",
      "Epoch [1/50], Step [69/3204], Train Loss: 0.1033 , Logging Time Proportion: 0.0150, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.102958083152771\n",
      "Epoch [1/50], Step [70/3204], Train Loss: 0.1030 , Logging Time Proportion: 0.0173, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.10166418552398682\n",
      "Epoch [1/50], Step [71/3204], Train Loss: 0.1017 , Logging Time Proportion: 0.0287, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.10261982679367065\n",
      "Epoch [1/50], Step [72/3204], Train Loss: 0.1026 , Logging Time Proportion: 0.0194, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.1004028469324112\n",
      "Epoch [1/50], Step [73/3204], Train Loss: 0.1004 , Logging Time Proportion: 0.0212, Data Loading Time Proportion: 0.0026\n",
      "loss:  0.10256870090961456\n",
      "Epoch [1/50], Step [74/3204], Train Loss: 0.1026 , Logging Time Proportion: 0.0210, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.10330602526664734\n",
      "Epoch [1/50], Step [75/3204], Train Loss: 0.1033 , Logging Time Proportion: 0.0170, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.09527188539505005\n",
      "Epoch [1/50], Step [76/3204], Train Loss: 0.0953 , Logging Time Proportion: 0.0221, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.09905318915843964\n",
      "Epoch [1/50], Step [77/3204], Train Loss: 0.0991 , Logging Time Proportion: 0.0214, Data Loading Time Proportion: 0.0019\n",
      "loss:  0.0980587750673294\n",
      "Epoch [1/50], Step [78/3204], Train Loss: 0.0981 , Logging Time Proportion: 0.0194, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.09738270938396454\n",
      "Epoch [1/50], Step [79/3204], Train Loss: 0.0974 , Logging Time Proportion: 0.0180, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.09268088638782501\n",
      "Epoch [1/50], Step [80/3204], Train Loss: 0.0927 , Logging Time Proportion: 0.0185, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.10105820000171661\n",
      "Epoch [1/50], Step [81/3204], Train Loss: 0.1011 , Logging Time Proportion: 0.0171, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.0955648273229599\n",
      "Epoch [1/50], Step [82/3204], Train Loss: 0.0956 , Logging Time Proportion: 0.0187, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.09837064146995544\n",
      "Epoch [1/50], Step [83/3204], Train Loss: 0.0984 , Logging Time Proportion: 0.0188, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.09560801833868027\n",
      "Epoch [1/50], Step [84/3204], Train Loss: 0.0956 , Logging Time Proportion: 0.0222, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.09428375214338303\n",
      "Epoch [1/50], Step [85/3204], Train Loss: 0.0943 , Logging Time Proportion: 0.0192, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.0931984931230545\n",
      "Epoch [1/50], Step [86/3204], Train Loss: 0.0932 , Logging Time Proportion: 0.0275, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.09716630727052689\n",
      "Epoch [1/50], Step [87/3204], Train Loss: 0.0972 , Logging Time Proportion: 0.0219, Data Loading Time Proportion: 0.0021\n",
      "loss:  0.09861481189727783\n",
      "Epoch [1/50], Step [88/3204], Train Loss: 0.0986 , Logging Time Proportion: 0.0182, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.09684993326663971\n",
      "Epoch [1/50], Step [89/3204], Train Loss: 0.0968 , Logging Time Proportion: 0.0210, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.09037207067012787\n",
      "Epoch [1/50], Step [90/3204], Train Loss: 0.0904 , Logging Time Proportion: 0.0308, Data Loading Time Proportion: 0.0026\n",
      "loss:  0.09393562376499176\n",
      "Epoch [1/50], Step [91/3204], Train Loss: 0.0939 , Logging Time Proportion: 0.0254, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.09560732543468475\n",
      "Epoch [1/50], Step [92/3204], Train Loss: 0.0956 , Logging Time Proportion: 0.0204, Data Loading Time Proportion: 0.0024\n",
      "loss:  0.09252528101205826\n",
      "Epoch [1/50], Step [93/3204], Train Loss: 0.0925 , Logging Time Proportion: 0.0211, Data Loading Time Proportion: 0.0028\n",
      "loss:  0.09433572739362717\n",
      "Epoch [1/50], Step [94/3204], Train Loss: 0.0943 , Logging Time Proportion: 0.0184, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.09801152348518372\n",
      "Epoch [1/50], Step [95/3204], Train Loss: 0.0980 , Logging Time Proportion: 0.0193, Data Loading Time Proportion: 0.0023\n",
      "loss:  0.09331917762756348\n",
      "Epoch [1/50], Step [96/3204], Train Loss: 0.0933 , Logging Time Proportion: 0.0242, Data Loading Time Proportion: 0.0025\n",
      "loss:  0.09016872197389603\n",
      "Epoch [1/50], Step [97/3204], Train Loss: 0.0902 , Logging Time Proportion: 0.0152, Data Loading Time Proportion: 0.0020\n",
      "loss:  0.09897132962942123\n",
      "Epoch [1/50], Step [98/3204], Train Loss: 0.0990 , Logging Time Proportion: 0.0234, Data Loading Time Proportion: 0.0018\n",
      "loss:  0.09081915020942688\n",
      "Epoch [1/50], Step [99/3204], Train Loss: 0.0908 , Logging Time Proportion: 0.0189, Data Loading Time Proportion: 0.0022\n",
      "loss:  0.09108337759971619\n",
      "Epoch [1/50], Step [100/3204], Train Loss: 0.0911 , Logging Time Proportion: 0.0278, Data Loading Time Proportion: 0.0020\n"
     ]
    }
   ],
   "source": [
    "trainer.train(t_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
